{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex 1 (4 pts). Logistic Regression (by hand)\n",
    "--\n",
    "\n",
    "Suppose we have the following normalized dataset\n",
    "\n",
    "$$\n",
    "\\begin{array}{lllr}\n",
    "GPA\t&GRE\t&Dummy\t&y \\\\ \\hline\n",
    "1\t&1\t&1\t&1 \\\\\n",
    "0.9\t&1\t&1\t&1 \\\\\n",
    "0.9\t&0.875\t&1\t&1 \\\\\n",
    "0.7\t&0.75\t&1\t&-1 \\\\\n",
    "0.6\t&0.875\t&1\t&-1 \\\\\n",
    "\\end{array}\n",
    "$$\t\t\t\n",
    "\t\t\t\n",
    "and want to construct a Logistic Regression classifier using Gradient Descent for Maximum Likelihood. If we start with an all zero weight vector, what will the weight vector be after the first iteration? (Consider kappa=2)\n",
    "Show the details of your calculations. This is a pencil and paper exercise. See the posted Excel spreadsheet for an example. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex 2 (10 pts). Logistic Regression - Gradient Descent\n",
    "--\n",
    "In this part you will build a logistic regression model using Numpy and doing gradient descent. \n",
    "You should complete the TODO parts in the following cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.     1.     1.    ]\n",
      " [1.     0.9    1.    ]\n",
      " [1.     0.9    0.875 ]\n",
      " [1.     0.7    0.75  ]\n",
      " [1.     0.6    0.875 ]\n",
      " [1.     0.6    0.875 ]\n",
      " [1.     0.5    0.75  ]\n",
      " [1.     0.5    0.8125]\n",
      " [1.     0.5    1.    ]\n",
      " [1.     0.5    0.875 ]\n",
      " [1.     0.5    0.875 ]]\n",
      "[[ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]]\n"
     ]
    }
   ],
   "source": [
    "# GPA, GRE scores dataset\n",
    "\n",
    "X=np.array([\n",
    "[1,1.0,1.0],\n",
    "[1,0.9,1.0],\n",
    "[1,0.9,0.875],\n",
    "[1,0.7,0.75],\n",
    "[1,0.6,0.875],\n",
    "[1,0.6,0.875],\n",
    "[1,0.5,0.75],\n",
    "[1,0.5,0.8125],\n",
    "[1,0.5,1.0],\n",
    "[1,0.5,0.875],\n",
    "[1,0.5,0.875]])\n",
    "\n",
    "print(X)\n",
    "\n",
    "y=np.array([[\n",
    "1,\n",
    "1,\n",
    "1,\n",
    "-1,\n",
    "-1,\n",
    "1,\n",
    "-1,\n",
    "-1,\n",
    "1,\n",
    "-1,\n",
    "1\n",
    "]]).T;\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a $w$ array with dimension 1 and X.shape[1] initialized to all ones, i.e. [[1,1,...,1]].\n",
    "\n",
    "Create a function to compute the loss (cost or error) function of logistic regression: \n",
    "$$\n",
    "E(w) = \\frac{1}{n} \\sum_{k=1}^n \\log(1+exp(-y^k*x^k @ w^T))\n",
    "$$\n",
    "where \"@\" is the matrix multiplication operator.\n",
    "Do not use loops. Utilize Numpy matrix operations. \n",
    "\n",
    "**Hints.**\n",
    "\n",
    "Create a function *error(x,y,w)* that returns $\\log(1+exp(-y*x@w^T))$. Use *np.log* and *np.exp*.\n",
    "\n",
    "Create a function *error_mean(X,y,w)* that calls *error(X,y,w)*. The latter returns an array of errors, one for each instance in X. Then sum these errors, and return the result divided by $n$. \n",
    "\n",
    "Do not hardcode $n$, but extract it from the shape of X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[[0.04858735]\n",
      " [0.05356278]\n",
      " [0.06048294]\n",
      " [2.53277152]\n",
      " [2.55580825]\n",
      " [0.08080825]\n",
      " [2.35020656]\n",
      " [2.40691288]\n",
      " [0.07888973]\n",
      " [2.46393947]\n",
      " [0.08893947]]\n",
      "1.1564462905916348\n"
     ]
    }
   ],
   "source": [
    "w = np.ones((1,X.shape[1]))\n",
    "\n",
    "#TODO\n",
    "def error(x,y,w):\n",
    "    return np.log(1 + np.exp(-y*x@w.T))\n",
    "\n",
    "print (error(X,y,w))\n",
    "\n",
    "#TODO\n",
    "def error_mean(X,y,w):\n",
    "    err = error(X,y,w)\n",
    "    return np.sum(err)/err.shape[0]\n",
    "\n",
    "\n",
    "print(error_mean(X,y,w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "\n",
    "1.15644629059"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to compute the gradient of the loss function at a point (given by w). \n",
    "Recall the the gradient function is:\n",
    "$$\n",
    "\\nabla_E(w) = -\\frac{1}{n} \\sum_{k=1}^{n} \\frac{y^k * x^k}{1+\\exp(y^k*x^k@w^T)}\n",
    "$$\n",
    "\n",
    "**Hints.**\n",
    "\n",
    "Create a function *grad(x,y,w)* that returns \n",
    "$$\\frac{y * x}{1+\\exp(y*x@w^T)}$$\n",
    "\n",
    "Create a function *grad_mean(X,y,w)* that calls *grad(X,y,w)* and computes and returns $\\nabla_E(w)$.\n",
    "\n",
    "Do not use loops. Do not hard-code $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04742587  0.04742587  0.04742587]\n",
      " [ 0.05215356  0.04693821  0.05215356]\n",
      " [ 0.05869017  0.05282116  0.0513539 ]\n",
      " [-0.92056145 -0.64439302 -0.69042109]\n",
      " [-0.92237054 -0.55342232 -0.80707422]\n",
      " [ 0.07762946  0.04657768  0.06792578]\n",
      " [-0.90465054 -0.45232527 -0.6784879 ]\n",
      " [-0.90990701 -0.4549535  -0.73929944]\n",
      " [ 0.07585818  0.03792909  0.07585818]\n",
      " [-0.91490095 -0.45745048 -0.80053834]\n",
      " [ 0.08509905  0.04254952  0.07446166]]\n",
      "[0.37959402 0.20802755 0.30424018]\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "def grad(x,y,w):\n",
    "    return (y*x)/(1+np.exp(y*x@w.T))\n",
    "\n",
    "print ((y*X)/(1+np.exp(y*X@w.T)))\n",
    "\n",
    "#TODO\n",
    "def grad_mean(X,y,w):\n",
    "    gra = grad(X,y,w)\n",
    "    return -np.sum(gra,axis=0)/gra.shape[0]\n",
    "\n",
    "print(grad_mean(X,y,w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "[ 0.37959402  0.20802755  0.30424018]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the gradient descent algorithm for logistic regression. \n",
    "\n",
    "For this create a function *fit(X,y,kappa,iter)* starts with an all-zero array for $w$ and improves it using *iter* iterations. Record in a list E the value of the *error_mean(X,y,w)* after each iteration. Return w,E. \n",
    "\n",
    "Plotting list E should give a decreasing curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.98019655  2.00560449  1.0649168 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fit(X,y,kappa,iter):\n",
    "    w = np.zeros((1,X.shape[1]))\n",
    "    E = []\n",
    "    \n",
    "\n",
    "    #TODO\n",
    "    for i in range(iter):\n",
    "        w = w - kappa * grad_mean(X,y,w)\n",
    "        E.append(error_mean(X,y,w))\n",
    "    \n",
    "    return w,E\n",
    "\n",
    "w,E = fit(X,y,1,100)\n",
    "print(w)\n",
    "plt.plot(E)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "[[-1.98019655  2.00560449  1.0649168 ]]\n",
    "\n",
    "and a decreasing curve of errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex 3 (10 pts). Logistic regression - Application\n",
    "--\n",
    "In the first part of this exercise, we'll build a logistic regression model to predict whether a student gets admitted to a university. Suppose that you are the administrator of a university department and you want to determine each applicant's chance of admission based on their results on two exams. You have historical data from previous applicants that you can use as a training set for logistic regression. For each training example, you have the applicant's scores on two exams and the admissions decision. To accomplish this, we're going to build a classification model that estimates the probability of admission based on the exam scores.\n",
    "\n",
    "Let's start by examining the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('ex3data1.txt', header=None, names=['Exam 1', 'Exam 2', 'Admitted'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use .values to obtain the underlying numpy array of the dataframe\n",
    "# we use iloc in order to access parts of the dataframe by using ranges; -1 is the last column\n",
    "X = data.iloc[:,0:-1].values\n",
    "\n",
    "# we normalize X\n",
    "maxX = np.max(X, axis=0)\n",
    "minX = np.min(X, axis=0)\n",
    "X = (X-minX)/(maxX-minX)\n",
    "print( X )\n",
    "\n",
    "\n",
    "# we insert an all-ones column at index 0\n",
    "X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "print(X)\n",
    "\n",
    "y = data.iloc[:,-1:].values \n",
    "\n",
    "\n",
    "# we build a boolean index\n",
    "where_are_zeros = (y==0)\n",
    "y[where_are_zeros] = -1\n",
    "\n",
    "print(y)\n",
    "\n",
    "# uncomment the above printouts to see what's going one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make the above into a function to use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(data):\n",
    "    X = data.iloc[:,0:-1].values\n",
    "\n",
    "    # we normalize X\n",
    "    maxX = np.max(X, axis=0)\n",
    "    minX = np.min(X, axis=0)\n",
    "    X = (X-minX)/(maxX-minX)\n",
    "\n",
    "    # we insert an all-ones column at index 0\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "    y = data.iloc[:,-1:].values \n",
    "\n",
    "    where_are_zeros = (y==0)\n",
    "    y[where_are_zeros] = -1\n",
    "    \n",
    "    return X,y\n",
    "\n",
    "\n",
    "X,y = prepare(data)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize X and y. We can create a function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(X, y, col1=1, col2=2):\n",
    "    positive_indexes = np.where(y == 1)[0]  #only the first element of the result is needed, i.e. we do [0]\n",
    "    negative_indexes = np.where(y == -1)[0]\n",
    "\n",
    "    positive = X[positive_indexes]  # positive rows\n",
    "    negative = X[negative_indexes]  # negative rows\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "    ax.scatter(positive[:,1:2], positive[:,2:], s=50, c='b', marker='o', label='Positive')\n",
    "    ax.scatter(negative[:,1:2], negative[:,2:], s=50, c='r', marker='x', label='Negative')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('col' + str(col1))\n",
    "    ax.set_ylabel('col' + str(col2))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "visualize(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w,E = fit(X,y,1,1000)\n",
    "print(w)\n",
    "plt.plot(E)\n",
    "plt.show()\n",
    "\n",
    "# We should get a decreasing curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to write a function that will output predictions for a dataset X using our learned parameters theta. We can then use this function to score the training accuracy of our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "def predict(w, X):\n",
    "    return np.where(1/(1+np.exp(-w@X.T)) > 0.5, 1, -1).T\n",
    "\n",
    "#TODO\n",
    "def accuracy(y,y_pred):\n",
    "    return np.sum(y == y_pred)/y.shape[0]\n",
    "\n",
    "y_pred = predict(w,X)\n",
    "print( accuracy(y,y_pred) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: 0.89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(X,y,pct=80):\n",
    "    n = X.shape[0]\n",
    "    s = round(n * pct / 100)\n",
    "    \n",
    "    indices = np.random.permutation(n)\n",
    "    train_idx, test_idx = indices[:s], indices[s:]\n",
    "    \n",
    "    X_train, X_test = X[train_idx,:], X[test_idx,:]\n",
    "    y_train, y_test = y[train_idx,:], y[test_idx,:]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = split_train_test(X,y,pct=80)\n",
    "w,E = fit(X_train,y_train,1,1000)\n",
    "print(w)\n",
    "plt.plot(E)\n",
    "plt.show()\n",
    "y_pred = predict(w,X_test)\n",
    "print( accuracy(y_test,y_pred) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "\n",
    "[[-8.10599158  9.01450903  8.71087333]]\n",
    "\n",
    "A decreasing curve. \n",
    "\n",
    "Accuracy: 0.95\n",
    "\n",
    "Results may vary due to the randomized nature of the train/test split. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex. 3 (10 pts). Linear Regression\n",
    "--\n",
    "\n",
    "Extend the linear regression algorithm for Python on the slides to:\n",
    "\n",
    "1.\tRead the data from a file (regdata.csv)\n",
    "2.\tScale the attributes\n",
    "3.\tCompute the error at each iteration and save the error values in a list\n",
    "4.\tPlot the error list as a curve in the end\n",
    "5.\tFind a good learning rate based on the error curve.\n",
    "\n",
    "**Hints.**\n",
    "1. y in the slides of linear regression is a row matrix. So, when extracting it from the dataset be careful to turn it into a row matrix.\n",
    "2. Do not forget to add the \"dummy\" attribute (all ones) to X. \n",
    "3. For this dataset the kappa should be quite small in order to have a decreasing E. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Reading the data from the regdata.csv file\n",
    "data = pd.read_csv('regdata.csv', header=None, names=['GPA', 'Years of Experience', 'Salary'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(data):\n",
    "    X = data.iloc[:,0:-1].values\n",
    "\n",
    "    maxX = np.max(X, axis=0)\n",
    "    minX = np.min(X, axis=0)\n",
    "    meanX = np.mean(X, axis=0)\n",
    "    X = (X-meanX)/(maxX-minX)\n",
    "\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "    y = data.iloc[:,-1:].values\n",
    "    return X,y\n",
    "\n",
    "X,y = prepare(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_mean(X,y,w):\n",
    "    return (1/X.shape[0])*np.sum((y.T-w@X.T).T*X,axis=0,keepdims=True)\n",
    "\n",
    "def error_mean(X,y,w):\n",
    "    return (1/(2*X.shape[0])) * (np.sum((y.T-w@X.T)**2))\n",
    "\n",
    "def fit_b(X,y,kappa,iter):\n",
    "    w = np.zeros((1,X.shape[1]))\n",
    "    E = []\n",
    "    \n",
    "    for i in range(iter):\n",
    "        w = w + (kappa * grad_mean(X,y,w))\n",
    "        E.append(error_mean(X,y,w))\n",
    "        \n",
    "    return w,E\n",
    "\n",
    "w,E = fit_b(X,y,1,100)\n",
    "print(w)\n",
    "plt.plot(E)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
